{
  "title": "Ollama Chat Model node common issues",
  "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/common-issues",
  "content": "---\ntitle: Ollama Chat Model node common issues\ndescription: Documentation for common issues and questions in the Ollama Chat Model node in n8n, a workflow automation platform. Includes details of the issue and suggested solutions.\ncontentType: [integration, reference]\npriority: high\n---\n\n Ollama Chat Model node common issues\n\nHere are some common errors and issues with the Ollama Chat Model node and steps to resolve or troubleshoot them.\n\n Processing parameters\n\nThe Ollama Chat Model node is a sub-node. Sub-nodes behave differently than other nodes when processing multiple items using expressions.\n\nMost nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn.\n\nIn sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name.\n\n Can't connect to a remote Ollama instance\n\nThe Ollama Chat Model node supports Bearer token authentication for connecting to remote Ollama instances behind authenticated proxies (such as Open WebUI).\n\nFor remote authenticated connections, configure both the remote URL and API key in your Ollama credentials. \n\nFollow the Ollama credentials instructions for more information.\n\n Can't connect to a local Ollama instance when using Docker\n\nThe Ollama Chat Model node connects to a locally hosted Ollama instance using the base URL defined by Ollama credentials. When you run either n8n or Ollama in Docker, you need to configure the network so that n8n can connect to Ollama.\n\nOllama typically listens for connections on localhost, the local network address. In Docker, by default, each container has its own localhost which is only accessible from within the container. If either n8n or Ollama are running in containers, they won't be able to connect over localhost.\n\nThe solution depends on how you're hosting the two components.\n\n If only Ollama is in Docker\n\nIf only Ollama is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way).\n\nWhen running the container, publish the ports with the -p flag. By default, Ollama runs on port 11434, so your Docker command should look like this:\n\nWhen configuring Ollama credentials, the localhost address should work without a problem (set the base URL to http://localhost:11434).\n\n If only n8n is in Docker\n\nIf only n8n is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 on the host.\n\nIf you are running n8n in Docker on Linux, use the --add-host flag to map host.docker.internal to host-gateway when you start the container. For example:\n\nIf you are using Docker Desktop, this is automatically configured for you.\n\nWhen configuring Ollama credentials, use host.docker.internal as the host address instead of localhost. For example, to bind to the default port 11434, you could set the base URL to http://host.docker.internal:11434.\n\n If Ollama and n8n are running in separate Docker containers\n\nIf both n8n and Ollama are running in Docker in separate containers, you can use Docker networking to connect them.\n\nConfigure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way).\n\nWhen configuring Ollama credentials, use the Ollama container's name as the host address instead of localhost. For example, if you call the Ollama container my-ollama and it listens on the default port 11434, you would set the base URL to http://my-ollama:11434.\n\n If Ollama and n8n are running in the same Docker container\n\nIf Ollama and n8n are running in the same Docker container, the localhost address doesn't need any special configuration. You can configure Ollama to listen on localhost and configure the base URL in the Ollama credentials in n8n to use localhost: http://localhost:11434.\n\n<!-- vale from-microsoft.HeadingColons = NO -->\n Error: connect ECONNREFUSED ::1:11434\n<!-- vale from-microsoft.HeadingColons = YES -->\n\nThis error occurs when your computer has IPv6 enabled, but Ollama is listening to an IPv4 address.\n\nTo fix this, change the base URL in your Ollama credentials to connect to 127.0.0.1, the IPv4-specific local address, instead of the localhost alias that can resolve to either IPv4 or IPv6: http://127.0.0.1:11434.\n\n Ollama and HTTP/HTTPS proxies\n\nOllama doesn't support custom HTTP agents in its configuration. This makes it difficult to use Ollama behind custom HTTP/HTTPS proxies. Depending on your proxy configuration, it might not work at all, despite setting the HTTPPROXY or HTTPSPROXY environment variables.\n\nRefer to Ollama's FAQ for more information.",
  "category": "cluster-nodes",
  "nodeType": "ollama",
  "keywords": [
    "ollama",
    "chat",
    "model",
    "node",
    "common",
    "issues",
    "docker",
    "localhost",
    "container",
    "when",
    "running",
    "credentials",
    "http",
    "host"
  ],
  "fetchedAt": "2025-10-07T16:15:31.353Z",
  "hash": "d651ecd08c0ac6dedfbeb083d7c2b1c4"
}