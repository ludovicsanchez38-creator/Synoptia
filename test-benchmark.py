#!/usr/bin/env python3
"""
Script de benchmark pour Synoptia Workflow Builder
Tests automatis√©s avec m√©triques et rapport markdown
"""

import requests
import json
import time
from datetime import datetime
from typing import Dict, List
import statistics

# Configuration
BASE_URL = "https://builder.synoptia.fr"
AUTH = ("admin", "Pepette1710*")

# 10 prompts de test vari√©s
TEST_PROMPTS = [
    {
        "id": 1,
        "category": "Email",
        "prompt": "Cr√©e un workflow qui envoie un email tous les lundis matin √† 9h"
    },
    {
        "id": 2,
        "category": "RSS & Automation",
        "prompt": "Automatise la veille : r√©cup√®re un flux RSS, r√©sume avec IA et envoie vers Notion"
    },
    {
        "id": 3,
        "category": "Scraping",
        "prompt": "Scrape ce site web et stocke les donn√©es dans Google Sheets"
    },
    {
        "id": 4,
        "category": "Chatbot",
        "prompt": "Chatbot support client avec base de connaissances"
    },
    {
        "id": 5,
        "category": "Backup",
        "prompt": "Backup automatique MongoDB vers S3 tous les jours"
    },
    {
        "id": 6,
        "category": "Notification",
        "prompt": "Notification Slack quand nouvel email important arrive dans Gmail"
    },
    {
        "id": 7,
        "category": "AI Image",
        "prompt": "G√©n√®re des images avec DALL-E et stocke sur Google Drive"
    },
    {
        "id": 8,
        "category": "ETL",
        "prompt": "Pipeline ETL : r√©cup√®re donn√©es API, transforme avec IA, stocke dans PostgreSQL"
    },
    {
        "id": 9,
        "category": "Sentiment Analysis",
        "prompt": "Analyse le sentiment des commentaires r√©seaux sociaux et alerte si n√©gatif"
    },
    {
        "id": 10,
        "category": "Onboarding",
        "prompt": "Workflow d'onboarding client automatique avec emails et cr√©ation de comptes"
    }
]

class WorkflowTester:
    def __init__(self):
        self.results = []
        self.start_time = None
        self.end_time = None

    def test_workflow_generation(self, prompt_data: Dict) -> Dict:
        """Teste la g√©n√©ration d'un workflow"""
        print(f"\nüîÑ Test #{prompt_data['id']}: {prompt_data['category']}")
        print(f"   Prompt: {prompt_data['prompt']}")

        start = time.time()

        try:
            response = requests.post(
                f"{BASE_URL}/api/generate",
                auth=AUTH,
                json={
                    "message": prompt_data["prompt"],
                    "autoExecute": False
                },
                timeout=60
            )

            duration = time.time() - start

            if response.status_code == 200:
                data = response.json()

                result = {
                    "id": prompt_data["id"],
                    "category": prompt_data["category"],
                    "prompt": prompt_data["prompt"],
                    "success": data.get("success", False),
                    "workflow_generated": data.get("workflow") is not None,
                    "json_valid": self._validate_json(data.get("workflow")),
                    "duration": duration,
                    "node_count": len(data.get("workflow", {}).get("nodes", [])) if data.get("workflow") else 0,
                    "validation": data.get("validation", {}),
                    "score": data.get("validation", {}).get("score", 0),
                    "grade": data.get("validation", {}).get("grade", "F"),
                    "errors": data.get("validation", {}).get("errors", []),
                    "warnings": data.get("validation", {}).get("warnings", [])
                }

                print(f"   ‚úÖ G√©n√©r√© en {duration:.2f}s - Score: {result['score']}/100 ({result['grade']})")
                return result

            else:
                print(f"   ‚ùå Erreur HTTP {response.status_code}")
                return {
                    "id": prompt_data["id"],
                    "category": prompt_data["category"],
                    "prompt": prompt_data["prompt"],
                    "success": False,
                    "error": f"HTTP {response.status_code}",
                    "duration": duration
                }

        except Exception as e:
            duration = time.time() - start
            print(f"   ‚ùå Exception: {str(e)}")
            return {
                "id": prompt_data["id"],
                "category": prompt_data["category"],
                "prompt": prompt_data["prompt"],
                "success": False,
                "error": str(e),
                "duration": duration
            }

    def _validate_json(self, workflow: Dict) -> bool:
        """Valide que le workflow est un JSON valide avec structure n8n"""
        if not workflow:
            return False

        required_fields = ["nodes", "connections"]
        return all(field in workflow for field in required_fields)

    def run_all_tests(self):
        """Ex√©cute tous les tests"""
        print("=" * 60)
        print("üß™ SYNOPTIA WORKFLOW BUILDER - BENCHMARK")
        print("=" * 60)

        self.start_time = datetime.now()

        for prompt_data in TEST_PROMPTS:
            result = self.test_workflow_generation(prompt_data)
            self.results.append(result)
            time.sleep(1)  # Pause entre les tests

        self.end_time = datetime.now()

    def calculate_metrics(self) -> Dict:
        """Calcule les m√©triques globales"""
        successful = [r for r in self.results if r.get("success")]
        workflows_generated = [r for r in self.results if r.get("workflow_generated")]
        valid_json = [r for r in self.results if r.get("json_valid")]

        durations = [r["duration"] for r in self.results if "duration" in r]
        scores = [r["score"] for r in self.results if "score" in r and r["score"] > 0]

        grade_distribution = {}
        for r in self.results:
            grade = r.get("grade", "F")
            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1

        return {
            "total_tests": len(TEST_PROMPTS),
            "successful": len(successful),
            "success_rate": (len(successful) / len(TEST_PROMPTS)) * 100,
            "workflows_generated": len(workflows_generated),
            "valid_json": len(valid_json),
            "avg_duration": statistics.mean(durations) if durations else 0,
            "min_duration": min(durations) if durations else 0,
            "max_duration": max(durations) if durations else 0,
            "avg_score": statistics.mean(scores) if scores else 0,
            "min_score": min(scores) if scores else 0,
            "max_score": max(scores) if scores else 0,
            "grade_distribution": grade_distribution,
            "total_duration": (self.end_time - self.start_time).total_seconds()
        }

    def generate_report(self):
        """G√©n√®re un rapport markdown"""
        metrics = self.calculate_metrics()

        report = f"""# üìä WORKFLOW BUILDER - RAPPORT DE TESTS

**Date:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Dur√©e totale:** {metrics['total_duration']:.2f}s

---

## üìà M√âTRIQUES GLOBALES

| M√©trique | Valeur | Attendu | Status |
|----------|--------|---------|--------|
| **Tests ex√©cut√©s** | {metrics['total_tests']}/10 | 10/10 | {'‚úÖ' if metrics['total_tests'] == 10 else '‚ùå'} |
| **Taux de succ√®s** | {metrics['success_rate']:.1f}% | >80% | {'‚úÖ' if metrics['success_rate'] >= 80 else '‚ö†Ô∏è' if metrics['success_rate'] >= 60 else '‚ùå'} |
| **Workflows g√©n√©r√©s** | {metrics['workflows_generated']}/{metrics['total_tests']} | {metrics['total_tests']}/{metrics['total_tests']} | {'‚úÖ' if metrics['workflows_generated'] == metrics['total_tests'] else '‚ùå'} |
| **JSON valides** | {metrics['valid_json']}/{metrics['total_tests']} | {metrics['total_tests']}/{metrics['total_tests']} | {'‚úÖ' if metrics['valid_json'] == metrics['total_tests'] else '‚ùå'} |
| **Temps moyen** | {metrics['avg_duration']:.2f}s | <10s | {'‚úÖ' if metrics['avg_duration'] < 10 else '‚ö†Ô∏è'} |
| **Score moyen** | {metrics['avg_score']:.1f}/100 | >85/100 | {'‚úÖ' if metrics['avg_score'] >= 85 else '‚ö†Ô∏è' if metrics['avg_score'] >= 70 else '‚ùå'} |

---

## üéØ DISTRIBUTION DES GRADES

| Grade | Nombre | Pourcentage |
|-------|--------|-------------|
"""

        for grade in ['A+', 'A', 'B', 'C', 'D', 'F']:
            count = metrics['grade_distribution'].get(grade, 0)
            percentage = (count / metrics['total_tests']) * 100
            report += f"| **{grade}** | {count} | {percentage:.1f}% |\n"

        report += f"""
---

## üìã R√âSULTATS D√âTAILL√âS

| # | Cat√©gorie | Score | Grade | Dur√©e | Status |
|---|-----------|-------|-------|-------|--------|
"""

        for r in self.results:
            status = "‚úÖ" if r.get("success") else "‚ùå"
            score = r.get("score", 0)
            grade = r.get("grade", "F")
            duration = r.get("duration", 0)

            report += f"| {r['id']} | {r['category']} | {score}/100 | {grade} | {duration:.2f}s | {status} |\n"

        report += f"""
---

## üîç ANALYSE PAR PROMPT

"""

        for r in self.results:
            report += f"""
### Test #{r['id']}: {r['category']}

**Prompt:** `{r['prompt']}`

**R√©sultat:**
- Status: {'‚úÖ Success' if r.get('success') else '‚ùå Failed'}
- Score: {r.get('score', 0)}/100
- Grade: {r.get('grade', 'N/A')}
- Dur√©e: {r.get('duration', 0):.2f}s
- Nodes: {r.get('node_count', 0)}
"""

            if r.get('errors'):
                report += f"- ‚ùå Erreurs: {len(r['errors'])}\n"
                for err in r['errors'][:3]:  # Max 3 erreurs
                    report += f"  - {err}\n"

            if r.get('warnings'):
                report += f"- ‚ö†Ô∏è Warnings: {len(r['warnings'])}\n"

        report += f"""
---

## üí° RECOMMANDATIONS

"""

        if metrics['success_rate'] >= 80:
            report += "‚úÖ **Excellent!** Le syst√®me fonctionne tr√®s bien.\n"
        elif metrics['success_rate'] >= 60:
            report += "‚ö†Ô∏è **Bon mais peut mieux faire.** Quelques am√©liorations n√©cessaires.\n"
        else:
            report += "‚ùå **√Ä am√©liorer.** Le syst√®me n√©cessite des corrections importantes.\n"

        if metrics['avg_score'] < 85:
            report += f"\n‚ö†Ô∏è Score moyen ({metrics['avg_score']:.1f}) inf√©rieur √† l'objectif (85). V√©rifier la qualit√© des workflows g√©n√©r√©s.\n"

        if metrics['avg_duration'] > 10:
            report += f"\n‚ö†Ô∏è Temps moyen ({metrics['avg_duration']:.2f}s) sup√©rieur √† l'objectif (10s). Optimiser les performances.\n"

        report += f"""
---

## üìä COMPARAISON AVEC OBJECTIFS

| Objectif | Valeur actuelle | Cible | Atteint? |
|----------|-----------------|-------|----------|
| Taux de succ√®s | {metrics['success_rate']:.1f}% | 80% | {'‚úÖ' if metrics['success_rate'] >= 80 else '‚ùå'} |
| Score moyen | {metrics['avg_score']:.1f}/100 | 85/100 | {'‚úÖ' if metrics['avg_score'] >= 85 else '‚ùå'} |
| Temps moyen | {metrics['avg_duration']:.2f}s | <10s | {'‚úÖ' if metrics['avg_duration'] < 10 else '‚ùå'} |
| Grades A+/A | {metrics['grade_distribution'].get('A+', 0) + metrics['grade_distribution'].get('A', 0)} | >70% | {'‚úÖ' if (metrics['grade_distribution'].get('A+', 0) + metrics['grade_distribution'].get('A', 0)) / metrics['total_tests'] >= 0.7 else '‚ùå'} |

---

**Rapport g√©n√©r√© par test-benchmark.py**
"""

        return report

    def save_results(self):
        """Sauvegarde les r√©sultats"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSON brut
        json_file = f"tests/benchmark_results_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "metrics": self.calculate_metrics(),
                "results": self.results
            }, f, indent=2, ensure_ascii=False)

        print(f"\nüíæ R√©sultats JSON: {json_file}")

        # Rapport markdown
        report = self.generate_report()
        md_file = f"tests/benchmark_report_{timestamp}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"üìÑ Rapport markdown: {md_file}")

        # Affiche aussi le rapport
        print("\n" + "=" * 60)
        print(report)

def main():
    """Point d'entr√©e"""
    tester = WorkflowTester()

    # V√©rifie que le serveur est accessible
    try:
        response = requests.get(f"{BASE_URL}/health", auth=AUTH, timeout=5)
        if response.status_code != 200:
            print(f"‚ùå Serveur non accessible: {BASE_URL}")
            return
    except Exception as e:
        print(f"‚ùå Impossible de se connecter au serveur: {e}")
        return

    print(f"‚úÖ Serveur accessible: {BASE_URL}\n")

    # Lance les tests
    tester.run_all_tests()

    # Sauvegarde et affiche les r√©sultats
    tester.save_results()

    print("\n‚ú® Tests termin√©s!")

if __name__ == "__main__":
    main()
