const { QdrantClient } = require('@qdrant/js-client-rest');
const OpenAI = require('openai');
const crypto = require('crypto');
const fs = require('fs').promises;
const path = require('path');
require('dotenv').config();

const client = new QdrantClient({ url: process.env.QDRANT_URL || 'http://localhost:6333' });
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const WORKFLOWS_DIR = process.env.WORKFLOWS_BACKUP_DIR || path.join(__dirname, 'data', 'workflows_backup');
const BATCH_SIZE = 50; // Traiter par lots pour √©viter rate limits
const MAX_WORKFLOWS = 2000; // Limiter pour l'ingestion initiale

// Extraction des informations cl√©s d'un workflow
function extractWorkflowInfo(workflow) {
  const nodes = workflow.nodes || [];
  const nodeTypes = [...new Set(nodes.map(n => n.type).filter(Boolean))];
  const nodeNames = [...new Set(nodes.map(n => n.name).filter(Boolean))];

  // Extraire les int√©grations (enlever les prefixes n8n-nodes-base.)
  const integrations = nodeTypes
    .map(t => t.replace('n8n-nodes-base.', '').replace('n8n-nodes-langchain.', ''))
    .filter(t => !['stickyNote', 'noOp'].includes(t));

  // D√©terminer la complexit√©
  const nodeCount = nodes.length;
  let complexity = 'medium';
  if (nodeCount <= 5) complexity = 'simple';
  else if (nodeCount >= 16) complexity = 'complex';

  // Extraire les triggers
  const triggers = nodes.filter(n => n.type?.includes('trigger') || n.type?.includes('Trigger'));

  // G√©n√©rer une description enrichie
  const description = workflow.description || workflow.name || 'Workflow n8n';

  return {
    id: workflow.id,
    name: workflow.name || 'Unnamed workflow',
    description,
    nodeCount,
    complexity,
    integrations: integrations.slice(0, 10), // Limiter √† 10 int√©grations max
    nodeTypes: nodeTypes.slice(0, 10),
    triggers: triggers.map(t => t.type),
    connections: Object.keys(workflow.connections || {}).length,
    active: workflow.active || false
  };
}

// G√©n√©rer le texte pour embedding
function generateEmbeddingText(info) {
  return `
Workflow N8N: ${info.name}

Description: ${info.description}

Caract√©ristiques:
- Nombre de n≈ìuds: ${info.nodeCount}
- Complexit√©: ${info.complexity}
- Connexions: ${info.connections}
- Actif: ${info.active ? 'Oui' : 'Non'}

Int√©grations utilis√©es: ${info.integrations.join(', ')}

Types de n≈ìuds: ${info.nodeTypes.join(', ')}

Triggers: ${info.triggers.length > 0 ? info.triggers.join(', ') : 'Manuel'}

Ce workflow peut √™tre utilis√© pour: ${info.description}
  `.trim();
}

// Parcourir r√©cursivement les fichiers JSON
async function findWorkflowFiles(dir, maxFiles = MAX_WORKFLOWS) {
  const files = [];

  async function traverse(currentDir) {
    if (files.length >= maxFiles) return;

    const entries = await fs.readdir(currentDir, { withFileTypes: true });

    for (const entry of entries) {
      if (files.length >= maxFiles) break;

      const fullPath = path.join(currentDir, entry.name);

      if (entry.isDirectory()) {
        await traverse(fullPath);
      } else if (entry.isFile() && entry.name.endsWith('.json') && !entry.name.includes('backup')) {
        files.push(fullPath);
      }
    }
  }

  await traverse(dir);
  return files;
}

// Ingestion par lots
async function ingestBatch(workflows, batchIndex) {
  const points = [];

  for (const workflow of workflows) {
    try {
      const embedding = await openai.embeddings.create({
        model: 'text-embedding-3-large',
        input: generateEmbeddingText(workflow),
        dimensions: 3072
      });

      const id = crypto.randomUUID();

      points.push({
        id,
        vector: embedding.data[0].embedding,
        payload: {
          type: 'workflow_example',
          source: 'n8n-workflows-github',
          category: 'workflow_template',
          timestamp: Date.now(),
          ...workflow,
          keywords: [
            ...workflow.integrations.map(i => i.toLowerCase()),
            workflow.complexity,
            'n8n',
            'workflow',
            'automation'
          ]
        }
      });

    } catch (error) {
      console.error(`‚ö†Ô∏è  Erreur embedding pour ${workflow.name}: ${error.message}`);
    }
  }

  if (points.length > 0) {
    await client.upsert('synoptia_knowledge', { points });
    console.log(`‚úÖ Batch ${batchIndex + 1}: ${points.length} workflows ing√©r√©s`);
  }

  return points.length;
}

// Fonction principale
(async () => {
  try {
    console.log('üöÄ Ingestion des workflows n8n...\n');
    console.log(`üìÇ R√©pertoire: ${WORKFLOWS_DIR}`);
    console.log(`üìä Limite: ${MAX_WORKFLOWS} workflows\n`);

    // 1. Trouver tous les fichiers
    console.log('üîç Recherche des fichiers JSON...');
    const workflowFiles = await findWorkflowFiles(WORKFLOWS_DIR);
    console.log(`‚úÖ ${workflowFiles.length} workflows trouv√©s\n`);

    // 2. Lire et extraire les infos
    console.log('üìñ Extraction des m√©tadonn√©es...');
    const workflows = [];
    let skipped = 0;

    for (const file of workflowFiles) {
      try {
        const content = await fs.readFile(file, 'utf8');
        const workflow = JSON.parse(content);
        const info = extractWorkflowInfo(workflow);

        // Filtrer les workflows vides ou invalides
        if (info.nodeCount > 0 && info.integrations.length > 0) {
          workflows.push(info);
        } else {
          skipped++;
        }
      } catch (error) {
        console.error(`‚ö†Ô∏è  Erreur lecture ${path.basename(file)}: ${error.message}`);
        skipped++;
      }
    }

    console.log(`‚úÖ ${workflows.length} workflows valides (${skipped} ignor√©s)\n`);

    // 3. Statistiques
    const stats = {
      simple: workflows.filter(w => w.complexity === 'simple').length,
      medium: workflows.filter(w => w.complexity === 'medium').length,
      complex: workflows.filter(w => w.complexity === 'complex').length,
      avgNodes: (workflows.reduce((sum, w) => sum + w.nodeCount, 0) / workflows.length).toFixed(1),
      topIntegrations: {}
    };

    workflows.forEach(w => {
      w.integrations.forEach(i => {
        stats.topIntegrations[i] = (stats.topIntegrations[i] || 0) + 1;
      });
    });

    const topIntegrations = Object.entries(stats.topIntegrations)
      .sort((a, b) => b[1] - a[1])
      .slice(0, 10);

    console.log('üìä Statistiques:');
    console.log(`   Simple: ${stats.simple} | Moyen: ${stats.medium} | Complexe: ${stats.complex}`);
    console.log(`   Moyenne n≈ìuds: ${stats.avgNodes}`);
    console.log(`\nüîù Top 10 int√©grations:`);
    topIntegrations.forEach(([name, count]) => {
      console.log(`   ${name}: ${count} workflows`);
    });
    console.log('');

    // 4. Ingestion par lots
    console.log('üíæ Ingestion dans Qdrant...\n');
    let totalIngested = 0;

    for (let i = 0; i < workflows.length; i += BATCH_SIZE) {
      const batch = workflows.slice(i, i + BATCH_SIZE);
      const count = await ingestBatch(batch, Math.floor(i / BATCH_SIZE));
      totalIngested += count;

      // Pause pour √©viter rate limits
      if (i + BATCH_SIZE < workflows.length) {
        await new Promise(resolve => setTimeout(resolve, 2000));
      }
    }

    console.log(`\nüéâ ${totalIngested} workflows ing√©r√©s dans Qdrant!`);

    // 5. V√©rification
    const results = await client.scroll('synoptia_knowledge', {
      filter: {
        must: [
          { key: 'type', match: { value: 'workflow_example' } }
        ]
      },
      limit: 10
    });

    console.log(`\n‚úÖ V√©rification: ${results.points.length} workflows dans Qdrant`);

    if (results.points.length > 0) {
      console.log('\nüìã Exemples ing√©r√©s:');
      results.points.slice(0, 3).forEach(p => {
        console.log(`   - ${p.payload.name} (${p.payload.complexity}, ${p.payload.nodeCount} n≈ìuds)`);
      });
    }

  } catch(e) {
    console.error('‚ùå Erreur:', e.message);
    console.error(e.stack);
    process.exit(1);
  }
})();
